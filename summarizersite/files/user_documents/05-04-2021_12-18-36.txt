4AnatomyofaLearningAlgorithm
4.1BuildingBlocksofaLearningAlgorithm
Youmayhavenoticedbyreadingthepreviouschapterthateachlearningalgorithmwesaw
consistedofthreeparts:
1)
alossfunction;
2)
anoptimizationcriterionbasedonthelossfunction(acostfunction,forexample);and
3)
anoptimizationroutineleveragingtrainingdatato˝ndasolutiontotheoptimization
criterion.
Thesearethebuildingblocksofanylearningalgorithm.Yousawinthepreviouschapter
thatsomealgorithmsweredesignedtoexplicitlyoptimizeaspeci˝ccriterion(bothlinearand
logisticregressions,SVM).Someothers,includingdecisiontreelearningandkNN,optimize
thecriterionimplicitly.DecisiontreelearningandkNNareamongtheoldestmachine
learningalgorithmsandwereinventedexperimentallybasedonintuition,withoutaspeci˝c
globaloptimizationcriterioninmind,and(likeitoftenhappenedinscienti˝chistory)the
optimizationcriteriaweredevelopedlatertoexplainwhythosealgorithmswork.
Byreadingmodernliteratureonmachinelearning,youoftenencounterreferencesto
gradient
descent
or
stochasticgradientdescent
.Thesearetwomostfrequentlyusedoptimization
algorithmsusedincaseswheretheoptimizationcriterionisdi˙erentiable.
Gradientdescentisaniterativeoptimizationalgorithmfor˝ndingtheminimumofafunction.
To˝nda
local
minimumofafunctionusinggradientdescent,onestartsatsomerandom
pointandtakesstepsproportionaltothenegativeofthegradient(orapproximategradient)
ofthefunctionatthecurrentpoint.
Gradientdescentcanbeusedto˝ndoptimalparametersforlinearandlogisticregression,
SVMandalsoneuralnetworkswhichweconsiderlater.Formanymodels,suchaslogistic
regressionorSVM,theoptimizationcriterionis
convex
.Convexfunctionshaveonlyone
minimum,whichisglobal.Optimizationcriteriaforneuralnetworksarenotconvex,butin
practiceeven˝ndingalocalminimumsu˚ces.
Let'sseehowgradientdescentworks.
4.2GradientDescent
Inthissection,Idemonstratehowgradientdescent˝ndsthesolutiontoalinearregression
problem
1
.IillustratemydescriptionwithPythoncodeaswellaswithplotsthatshowhow
thesolutionimprovesaftersomeiterationsofgradientdescent.Iuseadatasetwithonly
1
Asyouknow,linearregressionhasaclosedformsolution.Thatmeansthatgradientdescentisnot
neededtosolvethisspeci˝ctypeofproblem.However,forillustrationpurposes,linearregressionisaperfect
problemtoexplaingradientdescent.
AndriyBurkov
TheHundred-PageMachineLearningBook-Draft
3
onefeature.However,theoptimizationcriterionwillhavetwoparameters:
w
and
b
.The
extensiontomulti-dimensionaltrainingdataisstraightforward:youhavevariables
w
(1)
,
w
(2)
,
and
b
fortwo-dimensionaldata,
w
(1)
,
w
(2)
,
w
(3)
,and
b
forthree-dimensionaldataandsoon.
Figure1:Theoriginaldata.TheY-axiscorrespondstothesalesinunits(thequantitywe
wanttopredict),theX-axiscorrespondstoourfeature:thespendingsonradioadsinM$.
Togiveapracticalexample,Iusetherealdataset(canbefoundonthebook'swiki)withthe
followingcolumns:theSpendingsofvariouscompaniesonradioadvertisingeachyearand
theirannualSalesintermsofunitssold.Wewanttobuildaregressionmodelthatwecan
usetopredictunitssoldbasedonhowmuchacompanyspendsonradioadvertising.Each
rowinthedatasetrepresentsonespeci˝ccompany:
CompanySpendings,M$Sales,Units
137.822.1
239.310.4
345.99.3
441.318.5
......
Wehavedatafor200companies,sowehave200trainingexamplesintheform
(
x
i
;y
i
)=
(
Spendings
i
;Sales
i
)
.Figure1showsallexamplesona2Dplot.
Rememberthatthelinearregressionmodellookslikethis:
f
(
x
)=
wx
+
b
.Wedon'tknow
whattheoptimalvaluesfor
w
and
b
areandwewanttolearnthemfromdata.Todothat,
AndriyBurkov
TheHundred-PageMachineLearningBook-Draft
4
welookforsuchvaluesfor
w
and
b
thatminimizethemeansquarederror:
l
def
=
1
N
N
X
i
=1
(
y
i

(
wx
i
+
b
))
2
:
Gradientdescentstartswithcalculatingthepartialderivativeforeveryparameter:
@l
@w
=
1
N
N
X
i
=1

2
x
i
(
y
i

(
wx
i
+
b
));
@l
@b
=
1
N
N
X
i
=1

2(
y
i

(
wx
i
+
b
))
:
(1)
To˝ndthepartialderivativeoftheterm
(
y
i

(
wx
+
b
))
2
withrespectto
w
Iappliedthe
chainrule
.Here,wehavethechain
f
=
f
2
(
f
1
)
where
f
1
=
y
i

(
wx
+
b
)
and
f
2
=
f
2
1
.To˝nd
apartialderivativeof
f
withrespectto
w
wehaveto˝rst˝ndthepartialderivativeof
f
with
respectto
f
2
whichisequalto
2(
y
i

(
wx
+
b
))
(fromcalculus,weknowthatthederivative
@
@x
x
2
=2
x
)andthenwehavetomultiplyitbythepartialderivativeof
y
i

(
wx
+
b
)
with
respectto
w
whichisequalto

x
.Sooverall
@l
@w
=
1
N
P
N
i
=1

2
x
i
(
y
i

(
wx
i
+
b
))
.Inasimilar
way,thepartialderivativeof
l
withrespectto
b
,
@l
@b
,wascalculated.
Gradientdescentproceedsin
epochs
.Anepochconsistsofusingthetrainingsetentirelyto
updateeachparameter.Inthebeginning,the˝rstepoch,weinitialize
2
w
 
0
and
b
 
0
.
Thepartialderivatives,
@l
@w
and
@l
@b
givenbyeq.1equal,respectively,

2
N
P
N
i
=1
x
i
y
i
and

2
N
P
N
i
=1
y
i
.Ateachepoch,weupdate
w
and
b
usingpartialderivatives.Thelearningrate

controlsthesizeofanupdate:
w
 
w


@l
@w
;
b
 
b


@l
@b
:
(2)
Wesubtract(asopposedtoadding)partialderivativesfromthevaluesofparametersbecause
derivativesareindicatorsofgrowthofafunction.Ifaderivativeispositiveatsomepoint
3
,
thenthefunctiongrowsatthispoint.Becausewewanttominimizetheobjectivefunction,
2
Incomplexmodels,suchasneuralnetworks,whichhavethousandsofparameters,theinitializationof
parametersmaysigni˝cantlya˙ectthesolutionfoundusinggradientdescent.Therearedi˙erentinitialization
methods(atrandom,withallzeroes,withsmallvaluesaroundzero,andothers)anditisanimportantchoice
thedataanalysthastomake.
3
Apointisgivenbythecurrentvaluesofparameters.
AndriyBurkov
TheHundred-PageMachineLearningBook-Draft
5
