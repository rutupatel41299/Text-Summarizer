Microsoft released Windows XP on Oct. 25, 2001. That same day,in what may be a record, the company posted 18 megabytes ofpatches on its Web site: bug fixes, compatibility updates, andenhancements. Two patches fixed important security holes. Orrather, one of them did, the other patch didn’t work. Microsoftadvised (and still advises) users to back up critical files beforeinstalling the patches. Buyers of the home version of WindowsXP, however, discovered that the system provided no way torestore these backup files if things went awry. As Microsoft’sonline Knowledge Base blandly explained, the special backupfloppy disks created by Windows XP Home “do not work withWindows XP Home.”Such slip-ups, critics say, are merely surface lapses—signsthat the software’s developers were too rushed or too careless tofix obvious defects. The real problems lie in software’s basicdesign, according to R. A. Downes of Radsoft, a software consulting firm. Or rather, its lack of design. Microsoft’s popularVisual Studio programming software is an example, to Downes’sway of thinking. Simply placing the cursor over the Visual Studio window, Downes has found, invisibly barrages the centralprocessing unit with thousands of unnecessary messages, eventhough the program is not doing anything.“It’s cataclysmic….It’stotal chaos,” he complains.The issue, in the view of Dan Wallach, a computer scientistat Rice University, is not the pointless churning of the processor—after all, he notes, “processing power is cheap.” Nor is Microsoftsoftware especially flawed, critics often employ the company’sproducts as examples more because they are familiar thanbecause they are unusually bad. Instead, in Wallach’s view, theblooming, buzzing confusion in Visual Studio and so manyother programs betrays how the techniques for writing softwarehave failed to keep up with the explosive increase in its complexity.Programmers write code in languages such as Java, C andC  , which can be read by human beings. Specialized programs34 TECHNOLOGY REVIEW July/August 2002 www.technologyreview.com“ THE ATTITUDE TODAY IS THAT YOU CAN WRITE ANY SLOPPYPIECE OF CODE AND THE COMPILER WILL RUN DIAGNOSTICS.”www.technologyreview.com TECHNOLOGY REVIEW July/August 2002 35First Aid for Faulty CodeSoftware quality has been so bad for so long,some engineers argue, that the only solutionsare litigation and legislation. More optimisticobservers believe that industry is slowlybeginning to adopt new practices and technological tools for making better software.Among them:Perspective-based review. Engineers whodevelop code don’t look at software in thesame way as the system administrators whomaintain it, the marketers who sell it, or theend users who put it to work. Yet, says SteveMcConnell of Construx Software, developmentteams rarely account for these diverse perspectives. Involving colleagues like businessmanagers, administrators, customer supportagents and user interface experts in softwaredesign meetings “is obvious when you thinkof it, but hardly used at all,” McConnell says.Shared vision. Incredibly, the purpose of newsoftware is often not clearly spelled outbefore programmers begin writing it. Indeed,it often changes in midstream as marketerscome up with wish lists, with predictably badresults. According to software quality trainersJim and Michele McCarthy, one of the keys toimproving software is for all parties to reachan agreement in advance on what they’redoing—“a single, explicit, universallyaccepted focus.”Correct by construction. Some languages,such as Ada, are designed so that programmers simply cannot commit certainmistakes. Under the Kestrel Institute’s“correct by construction” approach, programmers carefully design and assemblesoftware modules using special programming tools that prevent errors such asbuffer overflows. Similarly, recentimprovements in Java compilers havehelped automate the process of weedingout common problems in Java code.Tracking revisions. According to AmitabhSrivastava of Microsoft, improvementswill also come from new tools that meticulously tally changes in software code, allowing testers to focus on heavily rewritten sections that may contain moreerrors. These and other similar changes,he says, will reverse the now prevalentapproach of slapping components togetherby inspiration in offices full of pizza boxes andMountain Dew.known as “compilers” transform this code into the strings of onesand zeroes used by computers. Importantly, compilers refuse tocompile code with obvious problems—they spit out error messages instead. Until the 1970s, compilers sat on large mainframesthat were often booked days or weeks in advance. Not wantingerrors to cause delay, coders—who in the early days tended tobe trained as mathematicians or physicists—stayed late in theiroffices exhaustively checking their work. Writing software wasmuch like writing scientific papers. Rigor, documentation andpeer-review vetting were the custom.But as computers became widespread, attitudes changed.Instead of meticulously planning code, programmers stayed upin caffeinated all-night hacking sessions, constantly bouncingresults off the compiler. Again and again, the compiler would spitback error messages, the programmers would fix the mistakesone by one until the software compiled properly. “The attitudetoday is that you can write any sloppy piece of code and the compiler will run diagnostics,” says SRI’s Neumann.“If it doesn’t spitout an error message, it must be done correctly, right?”As programs grew in size and complexity, however, thelimits of this “code and fix” approach became evident. Onaverage, professional coders make 100 to 150 errors in everythousand lines of code they write, according to a multiyear studyof 13,000 programs by Humphrey of Carnegie Mellon. UsingHumphrey’s figures, the business operating system Windows NT4, with its 16 million lines of code, would thus have been written with about two million mistakes. Most would have been toosmall to have any effect, but some—many thou